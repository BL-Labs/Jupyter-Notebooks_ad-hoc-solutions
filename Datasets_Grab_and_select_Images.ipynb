{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPC7/LSXwMZAOrNaHW2HCr6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BL-Labs/Jupyter-Notebooks_ad-hoc-solutions/blob/main/Datasets_Grab_and_select_Images.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install icrawler"
      ],
      "metadata": {
        "id": "gQ1e-g6-wI3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import base64\n",
        "from icrawler import ImageDownloader\n",
        "from icrawler.builtin import GoogleImageCrawler\n",
        "from six.moves.urllib.parse import urlparse\n",
        "from IPython.core.display import HTML\n",
        "import time"
      ],
      "metadata": {
        "id": "Wg0yvT879a96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlsVMzUFwDxH"
      },
      "outputs": [],
      "source": [
        "def get_file_urls(words):\n",
        "\n",
        "  class CustomLinkPrinter(ImageDownloader):\n",
        "      file_urls = []\n",
        "\n",
        "      def get_filename(self, task, default_ext):\n",
        "          file_idx = self.fetched_num + self.file_idx_offset\n",
        "          return '{:04d}.{}'.format(file_idx, default_ext)\n",
        "\n",
        "      def download(self, task, default_ext, timeout=5, max_retry=3, overwrite=False, **kwargs):\n",
        "          file_url = task['file_url']\n",
        "          filename = self.get_filename(task, default_ext)\n",
        "\n",
        "          task['success'] = True\n",
        "          task['filename'] = filename\n",
        "\n",
        "          if not self.signal.get('reach_max_num'):\n",
        "              self.file_urls.append(file_url)\n",
        "\n",
        "          self.fetched_num += 1\n",
        "\n",
        "          if self.reach_max_num():\n",
        "              self.signal.set(reach_max_num=True)\n",
        "\n",
        "          return\n",
        "\n",
        "  google_crawler = GoogleImageCrawler(\n",
        "  downloader_cls=CustomLinkPrinter,\n",
        "  downloader_threads=1)\n",
        "\n",
        "  google_crawler.downloader.file_urls = []\n",
        "  #google_crawler.crawl(words, max_num=10, filters = dict(\n",
        "  #    license='commercial,modify'))\n",
        "  google_crawler.crawl(words, max_num=20)\n",
        "  return google_crawler.downloader.file_urls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"SP_DatasetRegistry.csv\"\n",
        "df = pd.read_csv(filename, low_memory=False)\n",
        "i = 0"
      ],
      "metadata": {
        "id": "ZZEzkdcMA0IB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate an index file\n",
        "\n",
        "i = 0\n",
        "\n",
        "file = open(\"index.html\",\"w\")\n",
        "\n",
        "file.write(\"<body style='padding-left: 20px;padding-top: 30px;'><h2>Select a Dataset:</h2><p/>\")\n",
        "\n",
        "while i < df.shape[0]:\n",
        "  htmlfile = df['Dataset Title'][i].replace(r'/','_').replace(r'\\\\','_').replace(r' ','_').replace(r':','_')\n",
        "  file.write(\"<a href='\"+htmlfile+\".html'>\"+df['Dataset Title'][i]+\"</a><br><br>\")\n",
        "  i += 1\n",
        "\n",
        "file.close()\n",
        "\n",
        "# GRAB the first 10 URLs for ALL the datasets, and save them into separate files\n",
        "\n",
        "i = 0\n",
        "\n",
        "while i < df.shape[0]:\n",
        "\n",
        "  htmlfile = df['Dataset Title'][i].replace(r'/','_').replace(r'\\\\','_').replace(r' ','_').replace(r':','_')\n",
        "\n",
        "  file = open(htmlfile+\".html\",\"w\")\n",
        "\n",
        "  words = df['Dataset Title'][i] + ' British Library'\n",
        "  file_urls = get_file_urls(words)\n",
        "\n",
        "  file.write('<center style=\"font-variant: small-caps;font-size: large;\"><h2>'+df['Dataset Title'][i]+'</h2><br/><strong>1</strong><br/><br/><img src=\"'+file_urls[0]+'\" style=\"width:400px;\"><br/>'+file_urls[0]+'<p/><br/><strong>2</strong><br/><br/><img src=\"'+file_urls[1]+'\" style=\"width:400px;\"><br/>'+file_urls[1]+'<p/><br/><strong>3</strong><br/><br/><img src=\"'+file_urls[2]+'\" style=\"width:400px;\"><br/>'+file_urls[2]+'<p/><br/><strong>4</strong><br/><br/><img src=\"'+file_urls[3]+'\" style=\"width:500px;\"><br/>'+file_urls[3]+'<p/><br/><strong>5</strong><br/><br/><img src=\"'+file_urls[4]+'\" style=\"width:400px;\"><br/>'+file_urls[4]+'<p/><br/><strong>6</strong><br/><br/><img src=\"'+file_urls[5]+'\" style=\"width:400px;\"><br/>'+file_urls[5]+'<p/><br/><strong>7</strong><br/><br/><img src=\"'+file_urls[6]+'\" style=\"width:400px;\"><br/>'+file_urls[6]+'<p/><br/><strong>8</strong><br/><br/><img src=\"'+file_urls[7]+'\" style=\"width:400px;\"><br/>'+file_urls[7]+'<p/><br/><strong>9</strong><br/><br/><img src=\"'+file_urls[8]+'\" style=\"width:400px;\"><br/>'+file_urls[8]+'<p/><br/><strong>10</strong><br/><br/><img src=\"'+file_urls[9]+'\" style=\"width:400px;\"><br/>'+file_urls[9]+'</center>')  \n",
        "  i += 1\n",
        "  time.sleep(2)\n",
        "  file.close()"
      ],
      "metadata": {
        "id": "9rxLQSb5Xvww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "!mkdir Pages\n",
        "!mv ./*.html ./Pages/\n",
        "\n",
        "shutil.make_archive('AllDSCoverImagesSelectPages', 'zip', './Pages', './')\n",
        "print()\n",
        "print('------------- All DONE! --------------------')\n",
        "print('Output file: AllDSCoverImagesSelectPages.zip')\n",
        "!rm -rf Pages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9nY7bBKbaGx",
        "outputId": "eb7abbec-3d21-4ba6-cd17-6268d1485a9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "------------- All DONE! --------------------\n",
            "Output file: AllDSCoverImagesSelectPages.zip\n"
          ]
        }
      ]
    }
  ]
}